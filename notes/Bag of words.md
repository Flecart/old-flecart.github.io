---
layout: page
permalink: notes/bag-of-words
tags: en
title: Bag of words
---

### Introduzione a bag of words
Faremo una introduzione di applicazione di [Na√Øve Bayes](/notes/na√Øve-bayes) applicato alla classificazione di documenti.

#### Setting del problema üü®+
Questo √® una parte che √® importante nel caso volessimo fare **document classification**. e simili,
In questa brevissima introduzione cerchiamo di calcolare

$$
\theta_{i, \text{word}, l} = P(X_{i} =  \text{word} | Y = l)
$$

Ossia quanto √® probabile che una parola sia word, che appaia alla posizione i, data la categoria $$l$$  del documento
Assumendo che non dipenda dalla posizione posso solamente **contare le parole per documento** fregandomene della posizione, questa √® l'idea che ha portato ai primi approcci in questo campo.

Assumiamo che siano anche **indipendenti alla posizione per semplicit√†**, che √® una assunzione molto forte perch√© proviamo a catalogare la classe solamente dalla frequenza delle parole


$$
\theta_{i, \text{word}, l} = \theta_{\text{word}, l}
$$


#### Note: assunzioni forti (2) üü©
Perch√© √® una assunzione molto forte, e irrealistica il fatto che siano:
1. Indipendenti dalla posizione
2. indipendenti fra di loro (√® molto chiaro che certe parole vanno pi√π insieme rispetto ad altri, quindi √® una assunzione chiaramente false).
Per√≤ √® didattica la cosa che un modello semplice come [Na√Øve Bayes](/notes/na√Øve-bayes) possa essere utilizzato per un problema del genere.

### Forward inference
Una volta creato tutta la matrice di pesi e possibilit√† (quanto √® correlato),
Per fare una forward inference non faccio altro che fare una *dot product* perch√© usiamo quella come metrica per fare inference.

#### Inference in breve üü•
<img src="/images/notes/Bag of words-1696859313605.jpeg" alt="Bag of words-1696859313605">


Calcolando la $$s_{k}$$ come in immagine sopra, posso avere una sorta di **caratterizzazione del tema/categoria**, in ogni singolo documento che ho.

Poi per fare inferenza vedo il documento attuale √® pi√π simile rispetto a quale.

#### Cosine similarity üü©
Questa √® una metrica molto utilizzata per dare una idea di **somiglianza di vettori**.

<img src="/images/notes/Bag of words-1696859419879.jpeg" alt="Bag of words-1696859419879">