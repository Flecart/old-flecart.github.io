---
layout: page
permalink: notes/reinforcement-learning,-a-introduction
tags: en
title: Reinforcement Learning, a introduction
---

Ripasso Prox: 40
Ripasso: May 20, 2023
Ultima modifica: April 10, 2023 2:32 PM
Primo Abbozzo: January 26, 2023 10:00 PM
Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘
Studi Personali: No

# Reinforcement Learning

## Introduzione

Una delle idee migliori riguardanti questo campo del reinforcement learning Ã¨ il focus sul processo decisionale del singolo agente, condizionato al reward che lâ€™ambiente esterno gli dÃ  (feedback). Il setting classico di questo genere di problemi Ã¨ un caso speciale della caratterizzazione presente in [lâ€™intelligenza](/notes/lâ€™intelligenza).

Abbiamo in questo caso un agente allâ€™interno del suo ambiente. Lâ€™agente Ã¨ in grado di interagire col suo ambiente attraverso alcune azioni ben definite, e lâ€™ambiente restituisce un feedback ad ogni azione. Lâ€™agente si regola di conseguenza, nel tentativo di massimizzare il reward che riceve.

Ãˆ da notare che questa impostazione Ã¨ molto diversa rispetto al machine learning classico, seppur si puÃ² comunque collocare al suo interno. Classifcamente nei modelli di machine learning supervised si cerca di minimizzare un errore con alcuni dataset etichettati, mentre qui non abbiamo nessuna etichetta, mentre nel unsupervised proviamo a trovare alcuni pattern nei dati, mentre qui non cerchiamo nessun pattern. Si potrebbe dire che questo sia un terzo paradigma di machine learning.

NOTA: questi appunti riassumono concetti dai primi 4 capitoli del Sutton and Barto 2020

### Un problema classico: n-bandit
Vedere [N-Bandit Problem](/notes/n-bandit-problem).

### Setting classico (Model Policy Reward)

Quando andiamo a parlare di Reinforcement learning andiamo a considerare un setting classico di agente che interagisce con un ambiente attraverso delle azioni, e lâ€™ambiente che risponde attraverso i reward. Lâ€™agente osserva quindi lo stato (se Ã¨ full-observable vede lo stato esterno, altrimenti partially observable vede solamente parte delle informazioni dello stato dellâ€™ambiente) e insieme al reward percepito prova a eseguire delle altre azioni.

<img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled">

Sono particolarmente importanti quindi 3 parole chiave utili per descrivere una delle 3 frecce in immagine

#### Model

Il modello dell'ambiente lo indichiamo anche come dinamica o sistema di transizione dellâ€™ambiente. nel modello sono definite tutte le distribuzioni di probabilitÃ  che portano uno stato a un altro: $$P(s'|s)$$, questo possiamo dire, ossia partendo da uno stato s, quanto Ã¨ probabile finire in uno stato sâ€™ ??

Questo Ã¨ quello che ci dice il modello.

#### Policy

La policy Ã¨ un indicatore delle azioni del singolo agente, ci dice quanto Ã¨ probabile che lâ€™agente esegua una certa azione, dato che sia sopra un certo stato s, lo indichiamo solitamente con $$\pi(a | s)$$. Nel caso in cui Ã¨ una policy deterministica, nel senso che a uno stato corrisponde uno e un solo azione, potremmo scrivere qualcosa del tipo $$\pi (s) = a$$

#### Reward

Il reward descrive il feedback che lâ€™ambiente ritorna al giocatore una volta che una azione Ã¨ stata eseguita, spesso lo indichiamo in questi modi


$$
r(s, a) \\ r(s, a, s') \\ r(s)
$$


A seconda di quanto vogliamo esprimere (quindi il reward atteso dopo aver fatto una azione da unc erto stato, il reward atteso dopo aver fatto una azione da un certo stato ed essere arrivati a un certo stao e cosÃ¬ via

#### The Value function
Associamo ad ogni stato un reward $$r$$, si avrÃ  una **history** ossia una sequenza di $$O, A, S$$ osservazione dall'ambiente, azione fatta e stato presente. Ad ogni stato avrÃ² un reward.
quindi

$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + r_{i + 1} + \dots | S_{j}]
$$

Ossia se io al passo $$i$$ sono sullo stato $$S_{j}$$ la value function per quello stato Ã¨ il valore atteso dei rewards tutti successivi.
Questo si puÃ² scrivere in maniera piÃ¹ compatta come

$$
v_{i}(S_{j}) = \mathbf{E} [r_{i} + v_{i+1}(S) | S_{j}]
$$

Con $$S$$ uno stato su cui puoi essere al passo successivo.

All components are functions: 
- Policies: $$\pi: S \rightarrow A$$ (or to probabilities over A) 
- Value functions: $$v: S \rightarrow R$$ 
- Models: $$m: S \rightarrow S$$ and/or $$r: S \rightarrow R$$ 
- State update: $$u: S \times O \rightarrow S$$

## Categorie di agenti
### Policy - Value categorization 
#### Value Based
ha solamente value based, la sua policy Ã¨ basata sul suo valore (in modo greedy va a cercare quale sia lo stato con valore maggiore)

#### Policy based
Il contrario, non ha value function, ma solamente la policy

#### Actor Critic
Ha entrambi, ha sia policy (l'attore) e il critico che cerca di aiutare 
### Models

#### Model free
Se hanno policy o value, ma non hanno **nessun modello sull'ambiente** in cui sono presenti
#### Model based
Hanno il modello dell'ambiente, e non necessariamente hanno policy o value function.

### Prediction and control
**Prediction** Ã¨ la capacitÃ  di sapere come sarÃ  il futuro
**Control** Ã¨ la capacitÃ  di ottimizzare la propria value function.
Solitamente sono molto legati fra di loro.

## Markov chains

Dovrebbe essere approfondito meglio in [Markov Chains](/notes/markov-chains)

### Markov property ðŸŸ©

Uno stato si puÃ² dire di godere della proprietÃ  di markov se, intuitivamente parlando, possiede giÃ  tutte le informazioni necessarie per predire lo stato successivo, ossia, supponiamo di avere la sequenza di stati $$(S_n)_{n \in \N}$$, allora si ha che $$P(S_k | S_{k-1}) = P(S_k|S_0S_1...S_{k - 1})$$, ossia lo stato attuale in Sk dipende solamente dallo stato precedente.

Normalmente poche cose nel mondo reale si possono dire puramente Markoviane, perÃ² non si puÃ² negare che Ã¨ un modello molto buono di partenza come modello di decisione.

ma potremmo sempre rendere Markoviano creando una nuova variabile che ci rappresenta tutta la storia (Ã¨ qualcosa che non ho capito molto bene, ma credo si possa fare senza probbi).

### Markov processes ðŸŸ¨-

Possiamo andare a definire un processo markoviano come un insieme di stati e il modello di transizione probabilistico: $$(S, P)$$, una coppia di stati e tutto il modello di transizione. mi sembra di aver letto che un processo markoviano sia molto buono per studiare i moti browniani in fisica. Praticamente a random abbiamo che ogni punto si puÃ² muovere

- Esempio di processo markoviano

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 1.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 1">


### Markov Reward Processes (!!!)

Quando andiamo a parlare di processo markoviano con reward indichiamo che associamo una funzione valore $$V(s)$$ che restituisce un certo valore a ogni stato. Di solito questo valore ci Ã¨ ignoto a noi agenti che seguiamo iil modello, quindi diventa un buon problema in questo setting provare a stimare il valore dello stato in seguito a numerose osservazioni. Solitamente non vogliamo considerare tutti i reward con lo stesso peso. Vorremmo avere anche a disposizione un parametro che ci indichi quanto siano importanti i reward subito di ora, e i reward nel futuro. Con questo indichiamo un **discount factor** $$\gamma$$

Un tale processo viene formalizzato tramite una quadrupla   $$S, P, R,\gamma$$, con s stati possibili, P il modello di transizoine e R la funzione che ritorna il reward per ogni stato.

Solitamente viene definito **state value function:**


$$
V(s) = \mathbb{E}[R_t + \gamma R_{t + 1} + \gamma^2 R_{t + 2} + ... |  s = s _{t}]
$$


La parte dentro il valore atteso Ã¨ solitamente indicata con $$G_t$$.

**Metodi di estimazione della funzione valore:**

Abbiamo abbastanza metodi per stimare il valore della funzione: metodi di sampling, metodi diretti (analitici) e metodi basati su programmazione dinamica.

Riguardo i metodi di sampling questi sono i piÃ¹ dinamici, nel senso che permettono lâ€™applicazione a piÃ¹ problemi possibili, in generale hanno una precisione che va nellâ€™ordine dell $$\dfrac{1}{\sqrt {n}}$$ anche se non so su quali basi in particolare.

I metodi diretti sono leggermente piÃ¹ lenti, perchÃ© si tratta di risolvere lâ€™inversa della matrice, cosa che va in $$O(n^3)$$. Il motivo di questo Ã¨ che possiamo **sfruttare la proprietÃ  di V**

Ossia possiamo dire che


$$
V_k(s_t) = \mathbb{E}[R_t + \gamma G_{t + 1}  |  s = s _{t}] = R(s)  + \gamma \sum_{s'}P(s'|s)V_{k -1}(s')
$$


Questa osservazione permette di sviluppare un algoritmo iterativo per stivare il V fino a convergenza (sul perchÃ© converge sicuramente guardare altro, prolly idea degli operatori di bellman puÃ² essere utile)

- Algoritmo iterativo per valutazione della MRP

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 2.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 2">


**Soluzione analitica:**

Utilizzando la stessa proprietÃ  (solamente ora scritta in modo analitico, possiamo risolverlo come se fosse una matrice

- Soluzione analitica

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 3.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 3">


### Markov Decision Process

Questo Ã¨ molto simile alla MRP, solo che ora introduciamo una policy, ossia una funzione che ci dica quanto Ã¨ probabile compiere una certa azione in un certo stato

$$(S, A, P, R, \gamma)$$, ossia ora abbiamo sia stato, sia azione possibile e la funzione di transizione deve contare entrambi: $$P(\cdot | s, a)$$, mentre le reward sono ancora come prima.

- S lâ€™insieme di stati possibili
- A lâ€™insieme di azioni possibili
- P probabilitÃ  di raggiungereun certo stato, dato uno stato inizial e una azione
- R reward di uno stato
- gamma: decadimento del reward.

Ãˆ da notare che se possediamo una policy, allora possiamo ridurci al caso di Markov Reward Process, infatti possiamo dire che


$$
R^\pi(s) = \sum_{a \in A}\pi(a | s)R(s) \\

P^\pi(s'|s) = \sum_{a \in A}  \pi(a | s) P(s'|s, a)
$$


Quindi data una policy possiamo utilizzare gli argomenti fatti di sopra e **riuscire a dare una valutazione di essa**

- Algoritmo iterativo DP per policy evaluation

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 4.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 4">


## Policy Search

Cerchiamo ora la policy migliore possibile da applicare a un MDP, questo Ã¨ il problema del **policy control** ora che sappiamo come fare **policy evalutation** Ã¨ il momento giusto per introdurre soluzioni a questo problema.

Una soluzione **naÃ¯ve** Ã¨ semplicemente enumerare tutte le policy e utilizzare lâ€™algoritmo di policy evaluation, poi andare a vedere quale sia la migliore. Questo Ã¨ molto dispendioso perchÃ© assumento che posso applicare tutto lâ€™insieme di azioni a tutti gli stati ho potenzialmente $$|S|^{|A|}$$ policy possibili,, che sono troppi , e troppo brutti.

Ãˆ bene raggiunti questo punto provare a dare alcune definizioni utili.

### Definitions: state-action-value, optimal value policy

Sia $$\pi$$ una policy e $$V^\pi$$ la evaluation di quella policy, allora possiamo andare a definire la **state-action-value** function in questo modo:


$$
Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a)V^\pi(s')
$$


ossia ci dice piÃ¹ o meno il valore atteso dellâ€™azione a un certo stato!

Possiamo anche definire la policy migliore:


$$
\pi^*(s) = argmax_{\pi} V^\pi(s)
$$


Ossia Ã¨ la policy che rende massimo il valore in qualunque stato!

### Policy iteration and his monotonicity

Una volta creato una policy iteration, Ã¨ una cosa molto sensata andare a definire una nuova policy $$\pi_{k + 1}$$ definita in questo modo:


$$
\forall s, \pi_{k + 1}(s) = argmax_a Q(s, a)
$$


Ossia andiamo proprio a crearci una nuova policy, cercando di rendere maggiore possibile il valore atteso a fare una certa azione a uno stato! Riusciremo a dimostrare che $$\forall s, V^{\pi_{k + 1}}(s) \geq V^{\pi_{k}}(s)$$

- Dimostrazione dal sutton e barto

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 5.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 5">

    lâ€™idea principalmente Ã¨ prendere sempre il massimo volta dopo volta, e dimostrarlo per induzione in praticaâ€¦ Anche non ho capito come formalizzare e non ho capito se posso trarne vantaggi didattici nella formalizzazione di questa merda


### Value Iteration

Lâ€™idea di value iteration Ã¨ sostituirla subito, cioÃ¨ non stare a sviluppare fino in fondo la value evaluation, ma aggiornare la policy subito dopo.

- Pseudocodice value iteration

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 6.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 6">


### Bellman operator

- Slide

<img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 7.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 7">


Si puÃ² dimostrare che questo operatore Ã¨ una contrazione, quindi value iteration converge qualunque sia il punto di partenza

Con questo operatore, possiamo anche riscrivere in modo migliore la **policy evaluation**

- Slide

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 8.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 8">


- Proof of contraction of bellman operator

    <img src="/images/notes/image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 9.png" alt="image/universita/ex-notion/Reinforcement Learning, a introduction/Untitled 9">