---
layout: page
permalink: notes/convolutional-nn
tags: italian
title: Convolutional NN
---

Ultima modifica: May 6, 2023 5:55 PM
Primo Abbozzo: March 18, 2023 9:48 PM
Stato: ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘ðŸŒ‘
Studi Personali: No

# Elementi di ripasso

# Convolutional NN

## Introduction

### The convolution operator ðŸŸ©-

Il prodotto di convoluzione Ã¨ matematicamente molto contorto, anche se nella pratica Ã¨ una cosa molto molto semplice. In pratica voglio calcolare il valore di un pixel in funzione di certi suoi vicini, moltiplicati per un **filter** che in pratica Ã¨ una matrice di pesi, che definisce un pattern lineare a cui sarei interessato di cercare nellâ€™immagine.

- Slides ed esempi (molto piÃ¹ chiari)

    Vedi che per calcolare quellâ€™8 sto facendo cose lineari con tutti pixel intorno ad essa.

    <img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled.png" alt="image/universita/ex-notion/Convolutional NN/Untitled">

    <img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 1.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 1">


Questo operatore lâ€™abbiamo giÃ  trattato in modo molto breve in [Deblur di immagini](/notes/deblur-di-immagini).

### Some properties and uses

Sappiamo tutti che le immagini non sono altro che arrai di valori in un certo intervallo, che rappresentano lâ€™intensitÃ  dei colori, o solamente del bianco-grigio nel caso delle immagini grigio nere.

Queste intensitÃ  si potrebbero anche rappresentare come superfici 3d in cui la posizione del pixel identifica x e y, mentre lâ€™intensitÃ  la z, abbiamo quindi proprio delle superfici!, delle montagne, valli fiumi etc. Le cose molto interessanti sono cambi di intensitÃ  improvvisi (con derivata molto alta) ossia i dirupi, le valli, questo cambio improvviso (il cambio di fase come dice Pedro di Master algorithm) Ã¨ classico anche in nautura, Ã¨ la parte con qualche informazione di interesse diciamo.

**THE IDEA OF DERIVATIVE FOR CHANGES**

- Slide finite approssimation of derivative

<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 2.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 2">

    h = 1 perchÃ© siamo in campo discreto (Ã¨ anche il minimo h che possiamo considerare in questo setting), quel filtro quindi ci Ã¨ utile per capire se ci sono dei cambi improvvisi.


Questa idea ci permette di costruire il kernel per identificare le linee (visible contours of the image), orizzontali (cambi direzione e avresti verticale). itâ€™s a **feature map**, of the image to some characteristic of the image. (ti dice se in questa zona Ã¨ presente, non câ€™Ã¨ , o câ€™Ã¨ lâ€™opposto del pattern che cercavi).

## Some architectures

### Deepwise separable convolution

### Inception architecture ðŸŸ¨

Andiamo a derfinire un modulo di inception (in cui va a fare in un certo senso scrambling, decomporre e recomporre dati, in che modo vanno ad estrarre delle features io non lo so!).

> Comunque questa Ã¨ lâ€™architettura classica, andare ad utilizzare reti convoluzionali e poi operarle con reti deep (alla fine non molto deep) in modo da collegargli insieme.
>
- Esempio di inception module

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 3.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 3">

  [https://www.youtube.com/watch?v=VxhSouuSZDY&ab_channel=Udacity](https://www.youtube.com/watch?v=VxhSouuSZDY&ab_channel=Udacity)


### Residual layers

**Residual learning** is the main concept of these networks, itâ€™s when we have a direct link with the beginning! In pratica diamo la possibilitÃ  al neurone di scegliere di non modificare o invece sÃ¬ lâ€™input credo, provo a chiedermi se posso avere un valore migliore di quanto ho attualmente con qualche peso.

- Structure of residual layer

[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)

<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 4.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 4">


Usually these links help the network learn (lesser vanishing gradient.

### Transfer learning

- Slide intuizione transfer learning
<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 5.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 5">

- expected graph with performance with transfer learning

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 6.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 6">


Comunque lâ€™intuizione principale del transfer learning Ã¨ lâ€™idea che i primi layers facciano una sorta di estrazione di features piÃ¹ ad alto livello utili poi ai layers di deep NN. Se questa prima parte lâ€™ho trainata su un corpus enorme, allora gli aspetti che Ã¨ riuscito a generalizzare potrebbero essere utili anche per altro, e quindi utilizzo i pesi trovati in questa rete anche per altro, senza problemi.

**Fine tune o finetuning** Ã¨ un pÃ² rischioso, faccio un freeze di una parte del network piÃ¹ larga, potrei andare a overfittare e fare cose simili! PerÃ² ha piÃ¹ senso, ci aiuta a rendere lâ€™intera architettura ancora piÃ¹ focussato in quello che vogliamo fare noi (in un certo senso forse dÃ  via alcune generalizzazioni inutili nel nostro dominio)

## Training of CNN

### Backpropagation ðŸŸ¨++

We can unroll the input and output layers as a single linear trasformation of a deep network (with weights adjusted accordingly).

- Intuition of unrolling

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 7.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 7">


But how do we unroll?? We can see everything as a matrix with $$[input\_size \times output\_size]$$ as you can see from the image in the toggle

- Slide convolution matrix of the weights

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 8.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 8">


After we have modelled this matrix, we can learn using standard backpropagation we have talked about in [Neural Networks](/notes/neural-networks).

Un problema per questo metodo Ã¨ la matrice Ã¨ **sparsa** se input Ã¨ molto largo, e kernel piccolo, avrei un  numero di zeri assurdo, quindi nemmeno molto efficiente da memorizzare in questo modo. (perÃ² possiamo computare in modo efficiente, ma questo non lo trattiamo).

Un altro aspetto di questa matrice Ã¨ la **ripetizione shiftata** dei pesi, che sono gli stessi in ogni colonna della matrice, ma solamente shiftato. Questo cambia il modo di fare update dei pesi, si utilizza lâ€™update con average pesato. fra le 4 computazioni delle 4 colonne in esempio.

### Transposed convolutions

Dopo che ho fatto troppo downsampling con le CNN, vorrei tornare sÃ¹ di dimensione (se per esempio un input Ã¨ unâ€™immagine. Trasposed convolutions ci permettono di tornare su di dimensione. (anche tecniche statistiche credo che funzionino).

- Slide transposed convolutions

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 9.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 9">
!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 10.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 10">


This technique is called transposed convolution because if we transpose the convolution matrix, we see that we are upscaling the input!. PerÃ² non ho capito in che modo funziona!

### Dilated convolutions ðŸŸ©

- Slide intuizione di questo

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 11.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 11">


Facciamo una specie di padding interno sul kernel (non vado a contare certe cose, perÃ² riesco a ingrandire la receptive field del mio network.

Ha piÃ¹ senso fare sta cosa quando sto analizzando HIGH RESOLUTION IMAGE in cui il valore dei pixel cambia molto poco.

Una differnza con le Transposed convolutions ðŸŸ¥ Ã¨ il fatto che quelle sono fatte sullâ€™input, questa la facciamo su come viene calcolato il kernel.

Sono molto utilizzate in **temporal convolution networks**, in cui provo a diluire volta per volta lo spazio allâ€™interno del kernel, anche se non so ancora perchÃ© va

- Slide temporal convolution network

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 12.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 12">


## Normalization layers

### Why normalization ðŸŸ¥

- Slide 2 reasons

!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 13.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 13">


Why is normalization a good idea :D?

- So the quantitative values are comparable from each other (e.g. ages and income)
- We want the output of the layers to be comparable from each other, the middle outputs are inputs for other layers!
- We can better control the activation layers. (non vogliamo che faccia come output NaN ðŸ˜Ÿ)
- Decoupling of the layers. (non dobbiamo andare ad imparare il range di input aspettato, dato che sarÃ  sempre data di stesso tipo)

### Batch Normalization ðŸŸ¥

This is the most common form of normalization (ma lâ€™idea Ã¨ sempre la stessa, computare varianza e media, e poi sottrarre media e dividere per varianza). La cosa in piÃ¹ Ã¨ che vengono aggiunte delle varianze e una media, per denormalizzare lâ€™output, in modo che abbia la forma dei dati migliore possibile.

- Slide batch normalization
!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 14.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 14">


### Other Normalization ðŸŸ¥

Potremmo provare a **normalizzare per canale**

- Slide normalizations

!!<img src="/images/notes/image/universita/ex-notion/Convolutional NN/Untitled 15.png" alt="image/universita/ex-notion/Convolutional NN/Untitled 15">