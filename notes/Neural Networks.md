---
layout: page
permalink: notes/neural-networks
tags: italian
title: Neural Networks
---

Ripasso Prox: 10
Ultima modifica: May 6, 2023 5:55 PM
Primo Abbozzo: February 16, 2023 3:04 PM
Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ—
Studi Personali: No

# Elementi di ripasso

## Introduction: a neuron

Questi sono cosettini ispirati dalla struttura dei neuroni nel nostro cervello, ma mi sembra di aver sentito che alla fine non câ€™entrino proprio niente.

### Structure

1. Linear + activation

In pratica possiamo dire che un neurone in questa parte di AI Ã¨ una funzione lineare (quindi che non fa altro che $$w^Tx + b$$ di solito indicata con $$z$$) ossia moltiplicazione lineare, e poi una funzione non lineare tra 0 e 1 che mi indica o meno se la cosa Ã¨ attivata o meno.

### Model

Architecture and parameters (the weights), identify a model.

## Training network tricks

### Activation functions

Solitamente le funzioni classiche per i network neurali sono sigmoid, tanh, e ReLU. La cosa brutta delle prime due Ã¨ **vanishing gradient**, perchÃ© se il valore Ã¨ molto grosso o molto piccolo, la derivata Ã¨ molto vicino allo 0, quindi Ã¨ molto difficile aggiornare.

Invece ReLU si comporta meglio da questo punto di vista, perchÃ© la sua derivata Ã¨ 0 se minore di 0 1 se maggiore.

### Input normalization

In un certo senso in questo modo abbiamo un pÃ² di tati che sono Normali gaussiani. Non ho capito ancora perchÃ© normale gaussiana sia una tipologia di dati che ci piace cosÃ¬ tanto. (il motivo che viene dato [in lezione]([https://www.youtube.com/watch?v=zUazLXZZA2U&list](https://www.youtube.com/watch?v=zUazLXZZA2U&list)=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) Ã¨ che Gradient Descent si comporta molto meglio per loss function che sono gaussiane, perchÃ© la direzione di discesa Ã¨ sempre quella, e non deve zigzagare).

### Weight initialization

Ci sono moltissimi modi per inizializzare i Weights, in modo che si eviti il problema di vanishing or exploding gradients. Lâ€™idea Ã¨ comunque tenere i valori vicini a 1 per evitare che esplodino, e inversamente proporzionali a n o funzioni di n, perchÃ© se n Ã¨ molto grosso potrebbe esplodere lo stesso.

Alcune inizializzazioni famose sono

- Xavier
- He
- (qualcosa che funziona per Sigmoid, (alcune funzionano a seconda dellâ€™activation function giusta)

Câ€™Ã¨ ne sono molte, non so se conviene lavorare sulla inizializzazione, non credo sia comunque buona spesa del tempo a capire queste.

### Optimization

Momentum, praticamente un gradient descent che tiene conto delle computazioni passate, e calcola la direzione anche secondo quelle (quindi se vado su e giÃ¹ e a destra sempre nelle iterazioni passate, andrÃ² a destra piÃ¹ spesso diciamo, questa Ã¨ lâ€™intuizione per questa idea).

Una cosa molto strana Ã¨ che il training delle NN Ã¨ molto stabile. CioÃ¨ vari un pÃ² lâ€™input e non varia molto lâ€™ouput!

Possibili motivi:

1. Weights
2. Loss function
3. Internal redundancy? cioÃ¨ ho troppi parametri e questo lo rende bello.(teoria del prof)

### Overfitting

- Slide ways to reduce overfitting 7

    <img src="/images/notes/image/universita/ex-notion/Neural Networks/Untitled.png" alt="image/universita/ex-notion/Neural Networks/Untitled">


Overfitting Ã¨ il drago del training classico del machine learning, molto simile a dire che la macchina sta **allucinando alcuni pattern** causati probabilmente dalla varianza dei dati, o anche dal fatto che alcuni casi positivi sono pochiâ€¦

Ãˆ comunque una cosa troppo specifica, perchÃ© significa che la macchina stia quasi imparando a memoria i casi,  dovrebbe provare a generalizzare, per farlo deve scordare dettagli non interessanti (che con overfitting puÃ² imparare) e imparare le cose importanti. PerÃ² i computer sono troppo bravi a memorizzare dettagli, a differenza di umani.

- Idea del dropout

    <img src="/images/notes/image/universita/ex-notion/Neural Networks/Untitled 1.png" alt="image/universita/ex-notion/Neural Networks/Untitled 1">


Lâ€™idea Ã¨ il fatto che il network deve risolvere il problema, anche se Ã¨ un pÃ² rotto, questo cerca di renderlo piÃ¹ robusto, e sembra funzionare molto bene.

### Kullback-Leibler Divergence

We want to measure the **distance between two** distributions, usually from a real distribution and the one we are predicting.
Vedere Entropy#Relative Entropy or Kullback-Leibler
    


Ãˆ una cosa che proviene dalla teoria dellâ€™informazione.

Per capire questo, Ã¨ molto importante andare a capire cosa sia la **cross-entropy** e questo Ã¨ un modo abbastanza naturale per capire quanto vicino Ã¨ una distribuzione, solitamente predetta, con quella del training data, si puÃ² comparare molto con **log-likelihood** loss function, si potrebbe dire che sia un caso particolare la log likelihood.

# Registro ripassi

| 01/04/23 | Non ci sto facendo troppa attenzione. |
| --- | --- |
|  |  |
|  |  |
tion, si potrebbe dire che sia un caso particolare la log likelihood.