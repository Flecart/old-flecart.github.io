---
layout: page
permalink: notes/logistic-regression
tags: italian
title: Logistic Regression
---

## Introduzione alla logistic regression
### Giustificazione del metodo
Questo √® uno dei modelli classici, creati da **Minsky** qualche decennio fa
In questo caso andiamo direttamente a computare il valore di $$P(Y|X)$$ durante l'inferenza, quindi si parla di modello **discriminativo**, seguendo il modello presente in Introduction to machine learning

#### Introduzione al problema
Supponiamo che
- $$Y$$ siano variabili booleane
- $$X_{i}$$ siano variabili continue
- $$X_{i}$$ siano indipendenti uno dall'altro.
- $$P(X_{i}| Y= k)$$ sono modellate tramite distribuzioni gaussiane $$\mathbb{N}(\mu_{ik}, \sigma_{i})$$ 
	- NOTA! la varianza non dipende dalle feature!, questo mi permetterebbe di poi togliere la cosa quadratico dopo, rendendo poi l'approssimazione lineare
	- Per esempio se utilizziamo nelle immagini, avrebbe senso normalizzare pixel by pixel, e non image wide con un unico valore, √® una assunzione, che se funziona dovrebbe poi far andare meglio la regressione logistica!
- $$Y$$ √® una distribuzione bernoulliana.

Ci chiediamo come √® fatto $$P(Y|X)$$?

#### Caratterizzazione di P(Y|X) üü®+
Proviamo a calcolare analiticamente come √® fatto $$P(Y|X)$$ usando le assunzioni di sopra
**Theorem**
Assunte le cose di sopra si avr√† che 

$$
P(Y=1| X= \left< x_{1},\dots, x_{n} \right> ) = \frac{1}{1 + \exp\left( w_{0} + \sum_{i} w_{i}x_{i} \right)}
$$


<img src="/images/notes/Logistic Regression-1697464292967.jpeg" alt="Logistic Regression-1697464292967">
Nella derivazione di sopra si ha che $$\pi = P(Y=1)$$

E poi sappiamo che 


$$
\ln \frac{P(X_{i} | Y=0)}{P(X_{i} | Y=1)} = \ln \frac{e^{-\frac{(X_{i} - \mu_{i0})^{2}}{2 \sigma_{0}^{2}}}}{ e^{-\frac{(X_{i} - \mu_{i_{1}})^{2}}{2 \sigma_{1}^{2}}}} = -\frac{(X_{i} - \mu_{i0})^{2}}{2 \sigma_{i}^{2}} + \frac{(X_{i} - \mu_{i1})^{2}}{2 \sigma_{i}^{2}}
$$


E si pu√≤ notare che poi abbiamo il risultato di sopra e diventa sensato avere la forma di Sigmoid, che esce in modo molto molto naturale

Dalla parte in blu capiamo che √® una cosa **lineare**, perch√© se √® maggiore di zero allora √® meglio la probabilit√† di stare da una parte rispetto all'altra.

#### Funzione di Sigmoid üü©
Logistic Regression-1697464563385.jpeg|400|Logistic Regression-1697464563385.jpeg|400
Questo ci d√† una motivazione del motivo per cui utilizziamo

$$
\text{ Funzione di sigmoid: }\sigma(x) = \frac{1}{1 + e^{-x}} 
$$

**Derivata**
Si pu√≤ calcolare che ha una derivata molto molto carina, ma √® anche il problema per cui esiste **vanishing gradient**.

$$
\sigma'(x) = \sigma(x) (1 - \sigma(x))
$$


Quindi diventa vero che

$$
P(Y=1|x,w) = \sigma\left( w_{0} + \sum_{i} w_{i}x_{i} \right)
$$


Possiamo scrivere la probabilit√† di ogni singolo campione come in figura sotto

#### Funzione di loss üü©
<img src="/images/notes/Logistic Regression-1697462930410.jpeg" alt="Logistic Regression-1697462930410">
Che sembra una cross-entropy classica, che per√≤ non ha una soluzione analitica, per questo motivo si utilizza **discesa del gradiente**.

### Ottimizzazione discesa del gradiente
#### Intuizione sul gradiente üü©
abbiamo alla fine che il gradiente √®


$$
\frac{\delta \mathcal{L}(w)}{\delta w_{i}} = \sum_{l} x^{l}_{i} \cdot (y^{l} - \alpha^{l})
$$

Perch√© gi√† $$(y - \alpha)$$ sta misurando in un certo senso la differenza (l'errore), e il prodotto lo sta legando all'input preciso, quindi √® molto bello quando la formula √® interpretabile in modo fisico quasi.

#### Calcolo del gradiente cross entropy üü®++

$$
a^{l} = \sigma\left( w_{0} + \sum_{i} x_{i}w_{i} \right) = \sigma(z)
$$


$$
\sum_{l} \log P(Y= y^{l} | x ^{l}, w) = \sum_{l} y^{l}\log(\alpha^{l}) + (1-  y^{l})(1 - \log(\alpha^{l}))
$$

Dalla formula di sopra riscritta in altro modo.

<img src="/images/notes/Logistic Regression-1697463186166.jpeg" alt="Logistic Regression-1697463186166">
Questo √® esattamente poi quanto sar√† fatto durante il percettrone, per l'aggiornamento delle variabili in quelle istanze.

#### Fase update del gradiente üü©
Una volta calcolato che il valore di update √® analiticamente uguale a 

$$
\frac{\delta \mathcal{L}(w)}{\delta w_{i}} = \sum_{l} (y^{l} - \alpha^{l}) x^{l}
$$

Possiamo usare questa per aggiornare il peso di $$w_{i}$$

**Update step:**

$$
w_{i} = w_{i} + \mu \frac{\delta \mathcal{L}(w)}{\delta w_{i}} 
$$


A volte viene aggiunto un **fattore di regolarizzazione** che fa diventare la regola di update come 

$$
w_{i} = w_{i} + \mu \frac{\delta \mathcal{L}(w)}{\delta w_{i}}  + \mu \lambda|w_{i}|
$$

Che implica il fatto che se abbiamo un singolo peso grande, far√† molta fatica ad esserci nel regolarizzatore (quindi ho meno varianza fra i pesi diciamo).


## Note di ripasso

| Data | Commenti |
| ---- | -------- |
|10/30/2023      | Ho provato a ricavarli entrambe le dimostrazioni un po' pi√π difficili di questo appunto, pi√π o meno ci siamo, quindi positivo          |
|12/26/2023 | Non credo sia importante alla fine, basta capire il funzionamento di base!?  |