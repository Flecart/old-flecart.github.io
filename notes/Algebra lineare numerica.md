---
layout: page
permalink: notes/algebra-lineare-numerica
tags: italian
title: Algebra lineare numerica
---

Ripasso Prox: 24
Ripasso: December 23, 2022
Ultima modifica: January 2, 2023 9:50 AM
Primo Abbozzo: September 22, 2022 12:03 PM
Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ•
Studi Personali: No


# Algebra lineare numerica
#### Immagini
Lab 2 images

## Metodo di gauss

Vogliamo cercare un metodo per calcolare soluzioni a sistemi di equazione del genere:

$$Ax = b$$, classico. Supponiamo che questo sistema abbia una soluzione.

Il nostro obiettivo sarebbe scomporre la matrice $$A = LU$$
 come prodotto di due matrici Lower triangular e Upper triangular.

### Algoritmo per matrici Upper e lower triangular ðŸŸ©

Nel caso io abbia una matrice Lower o Upper triangular, i sistemi nella forma $$Lx=b$$ sono risolvibili con un algoritmo abbastanza banale che qui non riporto (sostituzioni via viaâ€¦)

Il costo Ã¨ di $$O(n^2)$$ e non ti scordare il /2 perchÃ© a lei piace

### Scomposizione A = LU ðŸŸ©

Questo Ã¨ un algoritmo che necessita di una osservazione abbastanza difficile:

Noto che durante l'applicazione dellâ€™algoritmo di gauss, per creare il pivot su una colonna, Ã¨ esattamente come se moltiplicassi per una (matrice identitÃ  + fattori per la colonna di interesse).

Come in esempio

<img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled">

Si puÃ² notare inoltre che che lâ€™inverso di L1 Ã¨ in una forma molto bella, esattamente la stessa con numeri invertiti!, inoltre la moltiplicazione fra  Ln Ã¨ anchâ€™essa in una forma molto carina!!! Facilita molto il prodotto.
In questo modo mi creo una matrice L, in cui Ã¨ molto facile invertire.

costo $$O(n^3/3)$$  per la fattorizzazione LU e  poi n2 due volte per risolvere L e U

### Scomposizione con permutazione ðŸŸ©

Ogni tanto potrebbe capitare che ci sia il bisogno di permutare, altrimenti non ho nessuna soluzione, da notare che questo Ã¨ un passo all'interno dellâ€™algoritmo di gauss.

Inoltre scegliere il maggiore dovrebbe aiutare ad eliminare mini-errorini dovuti allâ€™imprecisione della somma floating point.

- Algoritmo

    <img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 1.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 1">


### Forma finale scomposizioni ðŸŸ©

Se utilizziamo una sorta di **pivoting parziale** ossia per ridurre gli errori scegliamo il massimo della colonna (come del resto fa lâ€™algoritmo di sopra) allora non abbiamo bisogno di permutare colonne), se vogliamo invece un **pivoting totale** allora dobbiamo permutare le colonne e la matrice diventa qualcosa del tipo

$$A = QLUP$$ con Q le permutazioni su riga, e P quelle su colonna (quelle su colonna non Ã¨ che ci importa molto, perchÃ© basta moltiplicare per $$P^T$$, ossia la sua inversa per x e si risolvono problemi di questo genereâ€¦ dato che alla fine $$PP^T = I$$

### Metodo di Cholesky ðŸŸ©

Se prendo una matrice A **simmetrica definita positiva** (ricorda analisi per questo), allora con questo metodo riesco a riscriverla nella forma $$A = LL^T$$, e so che la matrice L triangolare inferiore non Ã¨ singolare perchÃ© ho tutti gli autovalori positivi.

Questo metodo costa $$O(n^3/6)$$ leggermente piÃ¹ veloce della fattorizzazione di Gauss.

Una volta fatto la scomposizione poi va subito

$$Ly = b, L^Tx = y$$ e si risolve, sul come funziona l'algoritmo, per ora ci importa solamente che Ã¨ piÃ¹ veloce.

Se non Ã¨ simmetrico o non Ã¨ definita positiva il programma crasha (lel che non sa come funzioni la prof. Ã¨ un sapere, non un conoscere, come diceva Bononi)

### Matrici simmetriche semidefinite o definite positive ðŸŸ©

Se prendo un qualunque vettore e vale


$$
x^TAx \geq 0
$$


Oppure avendo autovalori sempre positivi sono maggiore di 0, si puÃ² vedere che Ã¨ equivalente.

Guarda [Massimi minimi multi-variabile](/notes/massimi-minimi-multi-variabile) nella sezione forme quadratiche per questo.

## Metodi iterativi

Sono buoni perchÃ© l'errore di questi Ã¨ molto piÃ¹ alto, ma spesso molto piÃ¹ veloce, mentre per i metodi diretti Ã¨ esattamente il contrario (errore basso, alta complessitÃ ).

In modo molto generale, possiamo definire un metodo iterativo come una funzione $$G$$ applicata all'input stesso: $$x_n = G(x_{n - 1})$$

### Note introduttive convergenza e iterazione

Questa sotto Ã¨ la definizione di **convergenza ad $$\alpha$$ con ordine $$p \geq 1$$**

<img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 2.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 2">

Spesso la convergenza Ã¨ fissata da una condizione di arresto (che puÃ² essere dipendente anche dal numero di iterazioni. Possiamo definire **C = fattore di convergenza** quando $$p = 1$$, perchÃ© piÃ¹ Ã¨ piccolo piÃ¹ converge in fretta. C'Ã¨ anche una relazione simile fra i p, piÃ¹ Ã¨ grande, in questo caso, piÃ¹ velocemente converge.

## Metodi iterativi stazionari

### Idee Generali ðŸŸ©

Questi sono nella forma  $$x _{k +1} = Hx_k + d_k$$ Poi scriverlo come una differenza di matrici in particolare vorre $$A = M - N$$ , dove $$M$$  sia non singolare (e possibilmente facilmente invertibile). Se ciÃ² Ã¨ possibile allora

$$Ax = b \implies Mx - Nx = b \implies x = M^{-1}Nx + M^{-1}b \implies x = Tx + c$$

Una volta avuto questa matrice $$T$$ possiamo riutilizzarla per ogni iterazione di successione!

Avremo che $$x_k = Tx_{k - 1} + c$$, e si ha che $$x* = \lim x_k$$ deve essere che


$$
x* = Tx* + c
$$


E vorremmo che questo metodo **converga per ogni input** quindi potrei mettere un x_0 a casissimo

Una cosa non espressa durante la lezione Ã¨ che $$N = M - A$$ quindi


$$
M^{-1}N = M^{-1}(M - A) = I - M^{-1}A
$$


Che ci permette di scrivere la stessa cosa sente fare utilizzo della matrice $$N$$ e secondo me Ã¨ piÃ¹ clean. Questa cosa Ã¨ presente nel libro.

### Condizione necessaria e sufficiente di convergenza ðŸŸ©

**Teorema sulla condizione di convergenza:**

<img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 3.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 3">

Questo risultato si puÃ² connettere col raggio spettrale e si puÃ² concludere che converge sse


$$
\rho(T) < 1
$$


<img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 4.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 4">

### Metodo di Jacobi ðŸŸ©

Sia $$D$$ la parte diagonale di $$A$$, allora lâ€™iterazione in questa forma converge e si chiama metodo di jacobi


$$
x_{k + 1} = (I -D^{-1}A)x_k + D^{-1}b
$$


Nota: attento che la prof non ha insegnato in questo modo! Quindi probabilmente se te la chiede diglielo nella forma che le piace,

$$M = D, N = E + F$$

### Metodo Gauss-Seidel ðŸŸ©

Si ricorda che $$A = D - E - F$$ in modo molto strano di scriverloâ€¦

Si fa uso della matrice Lower con anche la diagonale, per il resto Ã¨ esattamente uguale a tutto il resto per i metodi iterativi., quindi


$$
M = D - E, N = F
$$


### Condizioni di convergenza per Jacobi e Gauss-Seidel ðŸŸ¥

Per comprendere questa parte Ã¨ importante il concetto di matrice con diagonale dominante. In parole umane deve essere che ogni elemento sulla matrice deve essere maggiore della somma di tutto quanto non stia sulla diagonale maggiore.

- Slide condizioni

    <img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 5.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 5">


Nota questi sono in pratica da imparare a memoria, ma non me ne ricordo!

### Gradiente coniugato

Questo Ã¨ un metodo di arresto molto misterioso, direi di cacharla ðŸ¤‘.

<img src="/images/notes/image/universita/ex-notion/Algebra lineare numerica/Untitled 6.png" alt="image/universita/ex-notion/Algebra lineare numerica/Untitled 6">

## Metodi di cui non ha parlato

### Metodo SOR

### Metodi di Rilassamento

### Metodi di Krylov



# References