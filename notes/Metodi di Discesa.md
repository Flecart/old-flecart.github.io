---
layout: page
permalink: notes/metodi-di-discesa
tags: italian
title: Metodi di Discesa
---

Ripasso Prox: 13
Ripasso: December 23, 2022
Ultima modifica: January 3, 2023 11:12 AM
Primo Abbozzo: November 3, 2022 11:53 AM
Stato: ðŸŒ•ðŸŒ•ðŸŒ•ðŸŒ‘ðŸŒ‘
Studi Personali: No

# Elementi di ripasso

# Metodi di discesa

## Intro

### Generali sui metodi di discesa

Vogliamo creare algoritmi che riescano a trovare i punti di minimo delle funzioni non vincolate.

In generale **si trova un punto stazionario (condizioni necessarie)** ma non Ã¨ garantito lo stato ottimo.

<img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled">

### Condizioni di arresto classiche (2) ðŸŸ©-

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 1.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 1">

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 2.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 2">

    - Differenza fra due iterati Ã¨ minore a una tolleranza, in modo simile a quanto fatto in [Equazioni non lineari](/notes/equazioni-non-lineari) sulla tolleranza per il metodo delle approssimazioni

I criteri di arresto sono i sempre classici

1. Numero di iterazioni superate
2. Tolleranza sullâ€™obiettivo ossia **gradiente = 0** per il punto di stazionarietÃ .

### Idea principale dei metodi di discesa ðŸŸ©

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 3.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 3">

1. Trovare una direzione di discesa
2. Aggiornare x in modo da andare in quella direzione di un passo fissato. **direzione di ricerca e lunghezza del passo** sono due nuovi termini di interesse per questa roba

### Ottimizzazione non vincolata (non studiare)

In pratica questa parte Ã¨ un ripasso molto veloce di 2 mesi di analisi 2â€¦ E poi ovviamente fatti molto male!

Guarda [Massimi minimi multi-variabile](/notes/massimi-minimi-multi-variabile). In pratica ottimizzazione Ã¨ trovare il massimo o il minimo di una funzioneâ€¦

## Discesa

### Condizione per direzione di discesaðŸŸ©-

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 4.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 4">


Si puÃ² osservare che io stia proprio scendendo, in questo senso vado a cercare qualcosa che sia uguale a 0.

- Interpretazione geometrica della condizione di discesa

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 5.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 5">


Se forma angolo ottuso con il gradiente Ã¨ OK!

E guardare le curve di livello Ã¨ una cosa molto buona.

### Algoritmo generale di GDðŸŸ©

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 6.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 6">


Si puÃ² notare che la **scelta del passo** Ã¨ la parte critica di questo algoritmo. Scegliere un alpha fisso non garantisce la convergenza, devono essere soddisfatte certe proprietÃ .

## Ricerca dellâ€™alpha

### Scelta della alpha (linea esatta)ðŸŸ©-

- Definizione di funzione dipendente da alpha

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 7.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 7">


<img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 8.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 8">

Vado a scegliere lâ€™alpha in modo tale che **assuma il valore piÃ¹ piccolo possibile,** significa minimizzare questa funzione di alpha.

Solitamente si utilizza quando f Ã¨ in forma quadratica, per resto spesso non si utilizza (perchÃ© la f si calcola in modo molto veloce, di solito Ã¨ molto lento. **dimostrato converge!** punto minimo o massimo boh.

### linea inesatta ðŸŸ¨+

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 9.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 9">


Quindi inesatta perchÃ© devo trovare l'insieme di alpha buoni, non quell preciso

### Condizioni di wolfe ðŸŸ¨-

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 10.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 10">


Minore, inoltre minore di qualcosina..

### Armijo e backtrackingðŸŸ¨

- Intro a backtracking

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 11.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 11">

- backtracking

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 12.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 12">

- Lâ€™algoritmo

    **NOTA**: bisgona anche mettere una condizione di maxit, se si esce per quella allora Ã¨ perchÃ© non si puÃ² trovare! (molto probabilmente)

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 13.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 13">


## Condizioni di ottimalitÃ 

### Definizione ðŸŸ©

Quando sono in un minimo locale o globale per la nostra funzione.

Andiamo a definire una condizione di ottimalitÃ  di primo secondo oordine perchÃ© andiamo a guardare le derivata

### Condizione necessaria e sufficiente al primo e secondo ordine ðŸŸ©

**Primo ordine**

Questo non Ã¨ altro che il teorema di fermat del secondo ordine che puoi trovare qui [11.1.2 Condizione di stazionarietÃ  (fermat) !!!](/notes/11.1.2-condizione-di-stazionarietÃ -(fermat)-!!!)

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 14.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 14">


Non esiste una condizione sufficiente al primo ordine

**Secondo ordine**

Questo Ã¨ il modo con cui trovavamo i punti di minimo, Ã¨ **anche una condizione sufficiente se Ã¨ definita positiva**

### OttimalitÃ  per funzioni convesse ! ðŸŸ©

<img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 15.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 15">

## Funzioni quadratiche

Questo tipo di funzioni ci piace molto perchÃ© sono convesse e le funzioni convesse sono facili da analizzare per sta robba.

### Quadratiche convesse ðŸŸ¥

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 16.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 16">


Con $$Q$$ matrice simmetrica

Con $$q$$ convessa se la matrice Ã¨ semidefinita positiva, e convessa stretta se Ã¨ definita positiva.

Questa funzione ci puÃ² essere utile per la risoluzione dei  [Minimi quadrati](/notes/minimi-quadrati). Quando mi calcolavo la norma quadrata per quel problema, avevo una funzione quadratica convessa (quasi, credo che il termine noto non fa male)!

### Soluzioni eq. normali

- Slide

    <img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 17.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 17">


Con la roba di sopra dimostro anche che una soluzione ottima (ricorda che Ã¨ ottima anche una soluzione locale) esiste sempre.

## Metodo di newton puro

Lâ€™unica cosa che cambia nello step di newton Ã¨ che la **direzione di discesa Ã¨ calcolata in modo differente**, in particolare possiamo vedere che lo step sia:

$$p_k = H(x_k) ^{-1} \nabla f(x_k)$$, le ragioni sono sconosciute, e non Ã¨ conveniente in termini di tempo provare a capire questo. **lo step Ã¨ sempre di 1**.

- La velocitÃ  di convegenza Ã¨ **quadratica** come nel caso descritto per newton in [Equazioni non lineari](/notes/equazioni-non-lineari).

## Ripasso (roba di analisi)

### Derivata parziale

Guardare [Calcolo differenziale](/notes/calcolo-differenziale).

Ma Ã¨ una roba troppo base sta roba ðŸ˜‘

### Hessiana

Questa matrice ci da informazioni sulle **derivate seconde**

1. Matrice simmetrica
2. derivata ij, prima derivo su i, poi su j

Ricordarsi il teorema di schwarz, che Ã¨ sufficiente per dimostrare che la matrice Ã¨ simmetrica

### Jacobiana

Questa ci da informazioni sulle **derivate prime** per tutti i modi possibili!

$$f: \R^n \to \R^m$$, $$J(f) : \R^n \to \R^{m\times n}$$

1. In particolare sulle righe ho tutte le derivate parziali possibili
2. Sulle colonne ho la derivata fatta su tutte le funzioni possibili

### Analogo minimimo massimo

Se ho il massimo e voglio il minimo, basta costruirsi la funzione swappata


$$
\argmax f(x) = \argmin - f(x)
$$


### Teorema sulle funzioni differenziabili

[Calcolo differenziale](/notes/calcolo-differenziale) Spiega bene questo teorema, con

> Se le derivate parziali esistono e sono continue, allora la funzione si dice differenziabile con continuitÃ  C.
>

### Punti di minimo locali e globali

**Globale**

Quando Ã¨ minimo per tutto il dominio.

**Locale**

Se Ã¨ un punto di minimo globale con dominio ridotto ad un intorno (basta che esista un interno).

### Funzioni convesse

Questa parte Ã¨ trattata in analisi in [ConvessitÃ  (cenni)](Convessita%CC%80%20(cenni)%20b097c1a254f84bd3926b796fb1179c89.md)

- Slides

    !<img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 18.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 18">

- Slide

    !<img src="/images/notes/image/universita/ex-notion/Metodi di Discesa/Untitled 19.png" alt="image/universita/ex-notion/Metodi di Discesa/Untitled 19">




# References